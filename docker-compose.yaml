version: '3.8'

services:
  ollama-engine:
    image: ollama/ollama:latest
    container_name: ollama-engine
    ports:
      - "11435:11434"
    volumes:
      - ./ollama_data:/root/.ollama
    networks:
      - assistant-net
    entrypoint: [ "/bin/sh", "-c", "ollama serve & sleep 10 && ollama pull $${MODEL_NAME:-qwen3:latest} && wait" ]
    restart: always

  assistant-backend:
    build: ./backend
    container_name: assistant-backend
    command: python run.py
    depends_on:
      - ollama-engine
    environment:
      - AI_PROVIDER=${AI_PROVIDER:-gemini}
      - GEMINI_API_KEY=${GEMINI_API_KEY}
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - OPENAI_MODEL_NAME=${OPENAI_MODEL_NAME:-gpt-4-turbo-preview}
      - TELEGRAM_BOT_TOKEN=${TELEGRAM_BOT_TOKEN}
      - FB_PAGE_ACCESS_TOKEN=${FB_PAGE_ACCESS_TOKEN}
      - FB_VERIFY_TOKEN=${FB_VERIFY_TOKEN}
      - OLLAMA_URL=http://ollama-engine:11434
      - MODEL_NAME=${MODEL_NAME:-qwen3:latest}
    volumes:
      - ./backend:/app
      - ./backend/prompts:/app/prompts
      - ./backend/data:/app/data
    networks:
      - assistant-net
    restart: always

  assistant-frontend:
    image: nginx:alpine
    container_name: assistant-frontend
    ports:
      - "80:80"
    volumes:
      - ./frontend:/usr/share/nginx/html
      - ./frontend/nginx.conf:/etc/nginx/conf.d/default.conf
    depends_on:
      - assistant-backend
    networks:
      - assistant-net
    restart: always

networks:
  assistant-net:
    driver: bridge
